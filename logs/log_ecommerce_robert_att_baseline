nohup: ignoring input
01/04/2024 09:32:42 - WARNING - root -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of BertEstorCrf were not initialized from the model checkpoint at /mnt/storage/wubinghong.wbh/deepl/torch_ner_new/prev_trained_model/roberta-large-chinese and are newly initialized: ['estor.self_attention.out_proj.bias', 'estor.output_linear.bias', 'estor.attention.in_proj_weight', 'estor.reshape_linear.weight', 'estor.self_attention.in_proj_bias', 'estor.gate_linear.weight', 'estor.attention.out_proj.bias', 'classifier.weight', 'estor.ff_norm.bias', 'crf.start_transitions', 'estor.attention.out_proj.weight', 'estor.reshape_linear.bias', 'estor.ff_norm.weight', 'estor.feed_forward.2.bias', 'classifier.bias', 'estor.feed_forward.0.weight', 'estor.self_attention.in_proj_weight', 'estor.feed_forward.0.bias', 'estor.att_norm.weight', 'estor.attention.in_proj_bias', 'estor.self_attention.out_proj.weight', 'estor.gate_linear.bias', 'estor.output_linear.weight', 'estor.feed_forward.2.weight', 'estor.tag_embedding_layer.weight', 'estor.att_norm.bias', 'estor.positional_embedding.embeddings', 'crf.transitions', 'crf.end_transitions']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/04/2024 09:32:51 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, adv_epsilon=1.0, adv_name='word_embeddings', backbone='bert_estor_crf', cache_dir='', config_name='', contrastive_alpha=0.1, crf_learning_rate=0.001, data_dir='/mnt/storage/wubinghong.wbh/deepl/torch_ner_new/datasets/ecommerce/', device=device(type='cuda'), do_adv=False, do_eval=False, do_lower_case=True, do_predict=False, do_train=True, enumerate_mode='attention', eval_all_checkpoints=False, eval_max_seq_length=512, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', freeze_backbone_epoch=30, gate_dropout_rate=0.5, gate_scaling_rate=0.6, gradient_accumulation_steps=1, id2label={0: 'X', 1: 'B-HC', 2: 'B-HP', 3: 'I-HC', 4: 'I-HP', 5: 'O', 6: '[START]', 7: '[END]'}, id2tag={0: 'product', 1: 'brand'}, if_add_a_self_attention=True, if_contrastive_learn=True, if_merge_by_add=True, label2id={'X': 0, 'B-HC': 1, 'B-HP': 2, 'I-HC': 3, 'I-HP': 4, 'O': 5, '[START]': 6, '[END]': 7}, learning_rate=3e-05, local_rank=-1, logging_steps=-1, loss_type='ce', markup='bio', max_grad_norm=1.0, max_steps=-1, model_name_or_path='/mnt/storage/wubinghong.wbh/deepl/torch_ner_new/prev_trained_model/roberta-large-chinese', n_gpu=1, no_cuda=False, num_train_epochs=50.0, output_dir='/mnt/storage/wubinghong.wbh/deepl/torch_ner_new/outputs/ecommerce_output/bert_estor_crf', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, pos_learning_rate=5e-05, predict_checkpoints=0, save_steps=-1, seed=42, tag2id={'product': 0, 'brand': 1}, tagging_rate=1.0, task_name='ecommerce', tokenizer_name='', train_max_seq_length=128, warmup_proportion=0.1, weight_decay=0.01)
01/04/2024 09:32:51 - INFO - root -   Loading features from cached file /mnt/storage/wubinghong.wbh/deepl/torch_ner_new/datasets/ecommerce/cached_crf-train_roberta-large-chinese_128_ecommerce
01/04/2024 09:32:52 - INFO - root -   The model with entity_enumerator
/mnt/storage/wubinghong.wbh/project/pytorch-3.8/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
01/04/2024 09:32:52 - INFO - root -   ***** Running training *****
01/04/2024 09:32:52 - INFO - root -     Num examples = 3988
01/04/2024 09:32:52 - INFO - root -     Num Epochs = 50
01/04/2024 09:32:52 - INFO - root -     Instantaneous batch size per GPU = 24
01/04/2024 09:32:52 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 24
01/04/2024 09:32:52 - INFO - root -     Gradient Accumulation steps = 1
01/04/2024 09:32:52 - INFO - root -     Total optimization steps = 8350
01/04/2024 09:32:52 - INFO - root -   Backbone bert_estor_crf param is freeze
['X', 'B-HC', 'B-HP', 'I-HC', 'I-HP', 'O', '[START]', '[END]'] ['product', 'brand']

Epoch: 0/50
[Training] 1/167 [..............................] - ETA: 7:42  [ loss=61.6127 ][Training] 2/167 [..............................] - ETA: 6:16  [ loss=45.3618 ][Training] 3/167 [..............................] - ETA: 5:22  [ loss=30.2755 ][Training] 4/167 [..............................] - ETA: 4:57  [ loss=31.2435 ][Training] 5/167 [..............................] - ETA: 4:34  [ loss=31.7869 ][Training] 6/167 [>.............................] - ETA: 4:28  [ loss=32.7923 ][Training] 7/167 [>.............................] - ETA: 4:23  [ loss=26.9610 ][Training] 8/167 [>.............................] - ETA: 4:22  [ loss=27.3666 ][Training] 9/167 [>.............................] - ETA: 4:15  [ loss=23.5699 ][Training] 10/167 [>.............................] - ETA: 4:12  [ loss=25.4513 ][Training] 11/167 [>.............................] - ETA: 4:06  [ loss=26.0078 ][Training] 12/167 [=>............................] - ETA: 4:05  [ loss=25.6112 ][Training] 13/167 [=>............................] - ETA: 4:02  [ loss=22.4192 ][Training] 14/167 [=>............................] - ETA: 3:59  [ loss=21.3220 ][Training] 15/167 [=>............................] - ETA: 3:55  [ loss=21.9556 ][Training] 16/167 [=>............................] - ETA: 3:54  [ loss=21.2563 ][Training] 17/167 [==>...........................] - ETA: 3:54  [ loss=22.1869 ][Training] 18/167 [==>...........................] - ETA: 3:52  [ loss=20.7635 ][Training] 19/167 [==>...........................] - ETA: 3:49  [ loss=18.3458 ][Training] 20/167 [==>...........................] - ETA: 3:47  [ loss=18.8074 ][Training] 21/167 [==>...........................] - ETA: 3:44  [ loss=20.1207 ][Training] 22/167 [==>...........................] - ETA: 3:43  [ loss=19.8868 ]/mnt/storage/wubinghong.wbh/deepl/torch_ner_new/models/layers/crf.py:233: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
Traceback (most recent call last):
  File "main.py", line 565, in <module>
    main()
  File "main.py", line 505, in main
    global_step, tr_loss = train(args, train_dataset, model, tokenizer,tag2spans_list)
  File "main.py", line 188, in train
    loss.backward()
  File "/mnt/storage/wubinghong.wbh/project/pytorch-3.8/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/storage/wubinghong.wbh/project/pytorch-3.8/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
